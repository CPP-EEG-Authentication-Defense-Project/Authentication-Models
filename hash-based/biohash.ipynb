{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup\n",
    "\n",
    "Initial module setup."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da4f13ce619dc02a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dataclasses\n",
    "import typing\n",
    "import auth_biohash.hash\n",
    "import auth_biohash.random_token\n",
    "import feature_encoding.threshold\n",
    "\n",
    "from eeg_auth_models_framework import data, pre_process, features, processor\n",
    "from eeg_auth_models_framework.utils import conversion"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T07:57:20.411470Z",
     "start_time": "2024-03-02T07:57:20.399970Z"
    }
   },
   "id": "77dbb2b529fba557",
   "execution_count": 113
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Constants"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0aaec4b61b22133"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "AUTHENTICATION_THRESHOLDS = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "DATASET_SAMPLE_FREQ_HZ = 200\n",
    "DATA_CHANNEL_NAMES = ['T7','F8','Cz','P4']\n",
    "FREQUENCIES = [\n",
    "    pre_process.FrequencyBand(lower=8.0, upper=12.0, label='Alpha'),\n",
    "    pre_process.FrequencyBand(lower=12.0, upper=35.0, label='Beta'),\n",
    "    pre_process.FrequencyBand(lower=4.0, upper=8.0, label='Theta'),\n",
    "    pre_process.FrequencyBand(lower=35.0, upper=None, label='Gamma'),\n",
    "    pre_process.FrequencyBand(lower=None, upper=None, label='Raw'),\n",
    "]\n",
    "WINDOW_SIZE = 1200\n",
    "WINDOW_OVERLAP = 0\n",
    "REDUCTION_WINDOW_SIZE = 5\n",
    "BINARY_THRESHOLD = 0.5"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T07:57:20.426970Z",
     "start_time": "2024-03-02T07:57:20.421972Z"
    }
   },
   "id": "aeed6ee7832bc937",
   "execution_count": 114
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utilities"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13bda9cd27dc1c7d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class ThresholdTestSet:\n",
    "    threshold: str\n",
    "    positive_cases: typing.List[auth_biohash.hash.BioHash]\n",
    "    negative_cases: typing.List[auth_biohash.hash.BioHash]\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class SubjectTestSet:\n",
    "    subject_id: str\n",
    "    threshold_tests: typing.List[ThresholdTestSet]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87196d459e581a50"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Configuration"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ac0435ab0259cc6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "downloader = data.AuditoryDataDownloader()\n",
    "reader = data.AuditoryDataReader()\n",
    "converter = conversion.MNEDataFrameConverter(\n",
    "    channels=DATA_CHANNEL_NAMES, \n",
    "    sample_frequency=DATASET_SAMPLE_FREQ_HZ\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T07:57:20.442470Z",
     "start_time": "2024-03-02T07:57:20.428471Z"
    }
   },
   "id": "7579517532835c5a",
   "execution_count": 115
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Processing Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72fc4ed2b371256f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-Processing Steps"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "993a651822b6459e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pre_process_steps = pre_process.PreProcessingPipeline([\n",
    "    pre_process.EEGBandpassFilterStep(\n",
    "        FREQUENCIES,\n",
    "        converter\n",
    "    ),\n",
    "    pre_process.DataWindowStep(WINDOW_SIZE, WINDOW_OVERLAP)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T07:57:20.457970Z",
     "start_time": "2024-03-02T07:57:20.443972Z"
    }
   },
   "id": "b12f85a89a924fdf",
   "execution_count": 116
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Extraction Steps"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "297f5a69a316f5b8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "feature_extraction_steps = features.FeatureExtractPipeline([\n",
    "    features.StatisticalFeatureExtractor([\n",
    "        features.StatisticalFeature.MIN,\n",
    "        features.StatisticalFeature.MAX,\n",
    "        features.StatisticalFeature.MEAN,\n",
    "        features.StatisticalFeature.ZERO_CROSSING_RATE\n",
    "    ])\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T07:57:20.473469Z",
     "start_time": "2024-03-02T07:57:20.459971Z"
    }
   },
   "id": "8ead5ce50ad8e4a3",
   "execution_count": 117
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Processor"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6c2f7c7de2ccd2a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_processor = processor.DataProcessor(\n",
    "    pre_process=pre_process_steps,\n",
    "    feature_extraction=feature_extraction_steps\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T07:57:20.488971Z",
     "start_time": "2024-03-02T07:57:20.474971Z"
    }
   },
   "id": "96cda2e2a18aec21",
   "execution_count": 118
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Subject Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad82ca1877854ce6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_path = downloader.retrieve()\n",
    "subject_data_map = reader.format_data(data_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T07:57:21.062471Z",
     "start_time": "2024-03-02T07:57:20.490471Z"
    }
   },
   "id": "65c233cea5cfefda",
   "execution_count": 119
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Token Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c873a699c65e119"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "subject_tokens_map = {subject: auth_biohash.random_token.generate_token() for subject in subject_data_map}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T07:57:21.077971Z",
     "start_time": "2024-03-02T07:57:21.064970Z"
    }
   },
   "id": "2163f478debcf65d",
   "execution_count": 120
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5513fde2d0127e36"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=40114\n",
      "    Range : 0 ... 40113 =      0.000 ...   200.565 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "processed_data_map = {subject: data_processor.process(subject_data_map[subject]) for subject in subject_data_map}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T07:57:22.937970Z",
     "start_time": "2024-03-02T07:57:21.079970Z"
    }
   },
   "id": "45137623ff340cd6",
   "execution_count": 121
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hashing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "256aa1a8ad30843c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Token Normalization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "570588497a87dce4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def normalize_vectors(vectors: typing.List[np.ndarray], token: str) -> typing.List[np.ndarray]:\n",
    "    matrix_generator = auth_biohash.random_token.MatrixGenerator(token)\n",
    "    normalization = auth_biohash.hash.TokenMatrixNormalization(matrix_generator)\n",
    "    return [normalization.normalize(v) for v in vectors]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T07:57:22.953469Z",
     "start_time": "2024-03-02T07:57:22.938969Z"
    }
   },
   "id": "aa0bce2e3282d654",
   "execution_count": 122
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "normalized_data_map = {subject: normalize_vectors(processed_data_map[subject], subject_tokens_map[subject]) for subject in processed_data_map}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T07:57:24.255469Z",
     "start_time": "2024-03-02T07:57:22.955469Z"
    }
   },
   "id": "ffdcbb747d2b305e",
   "execution_count": 123
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject S01: 20 normalized feature vectors\n",
      "Subject S02: 20 normalized feature vectors\n",
      "Subject S03: 20 normalized feature vectors\n",
      "Subject S04: 20 normalized feature vectors\n",
      "Subject S05: 33 normalized feature vectors\n",
      "Subject S06: 20 normalized feature vectors\n",
      "Subject S07: 20 normalized feature vectors\n",
      "Subject S08: 20 normalized feature vectors\n",
      "Subject S09: 20 normalized feature vectors\n",
      "Subject S10: 20 normalized feature vectors\n",
      "Subject S11: 20 normalized feature vectors\n",
      "Subject S12: 20 normalized feature vectors\n",
      "Subject S13: 20 normalized feature vectors\n",
      "Subject S14: 20 normalized feature vectors\n",
      "Subject S15: 20 normalized feature vectors\n",
      "Subject S16: 20 normalized feature vectors\n",
      "Subject S17: 20 normalized feature vectors\n",
      "Subject S18: 20 normalized feature vectors\n",
      "Subject S19: 20 normalized feature vectors\n",
      "Subject S20: 20 normalized feature vectors\n"
     ]
    }
   ],
   "source": [
    "for subject, vectors in normalized_data_map.items():\n",
    "    print(f'Subject {subject}: {len(vectors)} normalized feature vectors')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T07:57:24.270970Z",
     "start_time": "2024-03-02T07:57:24.256469Z"
    }
   },
   "id": "9ff305324627783c",
   "execution_count": 124
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Encoding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6636cd49c79953a1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def hash_vectors(vectors: typing.List[np.ndarray], threshold: float) -> typing.List[auth_biohash.hash.BioHash]:\n",
    "    encoder = feature_encoding.threshold.ThresholdBinaryEncoder(BINARY_THRESHOLD)\n",
    "    return [auth_biohash.hash.BioHash.generate_hash(v, threshold, encoder) for v in vectors]\n",
    "\n",
    "\n",
    "def make_map_of_threshold_hashes(vectors: typing.List[np.ndarray]) -> typing.Dict[str, typing.List[auth_biohash.hash.BioHash]]:\n",
    "    result = {}\n",
    "    for threshold in AUTHENTICATION_THRESHOLDS:\n",
    "        result[str(threshold)] = hash_vectors(vectors, threshold)\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T07:57:24.286503Z",
     "start_time": "2024-03-02T07:57:24.272469Z"
    }
   },
   "id": "a3f0ae55255cbe4b",
   "execution_count": 125
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "subject_hashes_map = {subject: make_map_of_threshold_hashes(normalized_data_map[subject]) for subject in normalized_data_map}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T07:57:24.333016Z",
     "start_time": "2024-03-02T07:57:24.287504Z"
    }
   },
   "id": "f752b778aa5d07ef",
   "execution_count": 126
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject S01 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S02 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S03 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S04 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S05 BioHash counts (per threshold): 0.1 --> 33, 0.2 --> 33, 0.3 --> 33, 0.4 --> 33, 0.5 --> 33, 0.6 --> 33, 0.7 --> 33, 0.8 --> 33, 0.9 --> 33\n",
      "Subject S06 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S07 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S08 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S09 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S10 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S11 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S12 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S13 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S14 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S15 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S16 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S17 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S18 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S19 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S20 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n"
     ]
    }
   ],
   "source": [
    "for subject, hashes in subject_hashes_map.items():\n",
    "    hash_counts = ', '.join([f'{hash_threshold} --> {len(hash_instances)}' for hash_threshold, hash_instances in hashes.items()])\n",
    "    print(f'Subject {subject} BioHash counts (per threshold): {hash_counts}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T07:57:24.348529Z",
     "start_time": "2024-03-02T07:57:24.334017Z"
    }
   },
   "id": "1263bc6905481b3e",
   "execution_count": 127
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test Set Assembly"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "532edd13a1d7457e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def make_threshold_test_sets(hashes_map: typing.Dict[str, typing.Dict[str, typing.List[auth_biohash.hash.BioHash]]], \n",
    "                             target_subject: str) -> typing.List[ThresholdTestSet]:\n",
    "    threshold_test_sets: typing.Dict[str, ThresholdTestSet] = {\n",
    "        str(threshold): ThresholdTestSet(threshold=threshold, positive_cases=[], negative_cases=[]) \n",
    "        for threshold in AUTHENTICATION_THRESHOLDS\n",
    "    }\n",
    "    for subject in hashes_map:\n",
    "        for threshold in hashes_map[subject]:\n",
    "            test_set = threshold_test_sets[threshold]\n",
    "            if subject == target_subject:\n",
    "                test_set.positive_cases.extend(hashes_map[subject][threshold])\n",
    "            else:\n",
    "                test_set.negative_cases.extend(hashes_map[subject][threshold])\n",
    "    return list(threshold_test_sets.values())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "247b0c7a4a4fffa8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "subject_test_sets = [\n",
    "    SubjectTestSet(subject, make_threshold_test_sets(subject_hashes_map, subject)) \n",
    "    for subject in subject_hashes_map\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfc60522e59349bb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Execute Tests"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7640bc382687a81"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO: mix test data and get scores/accuracy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae0bcfac89488908"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
