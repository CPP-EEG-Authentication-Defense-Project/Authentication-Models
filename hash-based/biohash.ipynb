{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup\n",
    "\n",
    "Initial module setup."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da4f13ce619dc02a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dataclasses\n",
    "import typing\n",
    "import math\n",
    "import random\n",
    "import auth_biohash.hash\n",
    "import auth_biohash.random_token\n",
    "import feature_encoding.threshold\n",
    "\n",
    "from eeg_auth_models_framework import data, pre_process, features, processor\n",
    "from eeg_auth_models_framework.utils import conversion"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T03:18:31.809538Z",
     "start_time": "2024-03-03T03:18:31.804036Z"
    }
   },
   "id": "77dbb2b529fba557",
   "execution_count": 63
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Constants"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0aaec4b61b22133"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "AUTHENTICATION_THRESHOLDS = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "DATASET_SAMPLE_FREQ_HZ = 200\n",
    "DATA_CHANNEL_NAMES = ['T7','F8','Cz','P4']\n",
    "FREQUENCIES = [\n",
    "    pre_process.FrequencyBand(lower=8.0, upper=12.0, label='Alpha'),\n",
    "    pre_process.FrequencyBand(lower=12.0, upper=35.0, label='Beta'),\n",
    "    pre_process.FrequencyBand(lower=4.0, upper=8.0, label='Theta'),\n",
    "    pre_process.FrequencyBand(lower=35.0, upper=None, label='Gamma'),\n",
    "    pre_process.FrequencyBand(lower=None, upper=None, label='Raw'),\n",
    "]\n",
    "WINDOW_SIZE = 1200\n",
    "WINDOW_OVERLAP = 0\n",
    "BINARY_THRESHOLD = 50"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T03:18:31.856037Z",
     "start_time": "2024-03-03T03:18:31.847036Z"
    }
   },
   "id": "aeed6ee7832bc937",
   "execution_count": 64
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utilities"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13bda9cd27dc1c7d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class HashTest:\n",
    "    expected_result: bool\n",
    "    hashes: typing.Tuple[auth_biohash.hash.BioHash, auth_biohash.hash.BioHash]\n",
    "    \n",
    "    def run_test(self):\n",
    "        result = (self.hashes[0] == self.hashes[1])\n",
    "        if result != self.expected_result:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class ThresholdTestSet:\n",
    "    threshold: str\n",
    "    positive_cases: typing.List[auth_biohash.hash.BioHash]\n",
    "    negative_cases: typing.List[auth_biohash.hash.BioHash]\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class SubjectTestSet:\n",
    "    subject_id: str\n",
    "    threshold_tests: typing.List[ThresholdTestSet]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T03:18:31.871536Z",
     "start_time": "2024-03-03T03:18:31.857537Z"
    }
   },
   "id": "87196d459e581a50",
   "execution_count": 65
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Configuration"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ac0435ab0259cc6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "downloader = data.AuditoryDataDownloader()\n",
    "reader = data.AuditoryDataReader()\n",
    "converter = conversion.MNEDataFrameConverter(\n",
    "    channels=DATA_CHANNEL_NAMES, \n",
    "    sample_frequency=DATASET_SAMPLE_FREQ_HZ\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T03:18:31.887036Z",
     "start_time": "2024-03-03T03:18:31.873035Z"
    }
   },
   "id": "7579517532835c5a",
   "execution_count": 66
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Processing Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72fc4ed2b371256f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-Processing Steps"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "993a651822b6459e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pre_process_steps = pre_process.PreProcessingPipeline([\n",
    "    pre_process.EEGBandpassFilterStep(\n",
    "        FREQUENCIES,\n",
    "        converter\n",
    "    ),\n",
    "    pre_process.DataWindowStep(WINDOW_SIZE, WINDOW_OVERLAP)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T03:18:31.902535Z",
     "start_time": "2024-03-03T03:18:31.889035Z"
    }
   },
   "id": "b12f85a89a924fdf",
   "execution_count": 67
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Extraction Steps"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "297f5a69a316f5b8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "feature_extraction_steps = features.FeatureExtractPipeline([\n",
    "    features.StatisticalFeatureExtractor([\n",
    "        features.StatisticalFeature.MIN,\n",
    "        features.StatisticalFeature.MAX,\n",
    "        features.StatisticalFeature.MEAN,\n",
    "        features.StatisticalFeature.ZERO_CROSSING_RATE\n",
    "    ])\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T03:18:31.918036Z",
     "start_time": "2024-03-03T03:18:31.904036Z"
    }
   },
   "id": "8ead5ce50ad8e4a3",
   "execution_count": 68
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Processor"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6c2f7c7de2ccd2a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_processor = processor.DataProcessor(\n",
    "    pre_process=pre_process_steps,\n",
    "    feature_extraction=feature_extraction_steps\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T03:18:31.933536Z",
     "start_time": "2024-03-03T03:18:31.919535Z"
    }
   },
   "id": "96cda2e2a18aec21",
   "execution_count": 69
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Subject Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad82ca1877854ce6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_path = downloader.retrieve()\n",
    "subject_data_map = reader.format_data(data_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T03:18:32.354538Z",
     "start_time": "2024-03-03T03:18:31.935038Z"
    }
   },
   "id": "65c233cea5cfefda",
   "execution_count": 70
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Token Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c873a699c65e119"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "subject_tokens_map = {subject: auth_biohash.random_token.generate_token() for subject in subject_data_map}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T03:18:32.384536Z",
     "start_time": "2024-03-03T03:18:32.364036Z"
    }
   },
   "id": "2163f478debcf65d",
   "execution_count": 71
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5513fde2d0127e36"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=40114\n",
      "    Range : 0 ... 40113 =      0.000 ...   200.565 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=24000\n",
      "    Range : 0 ... 23999 =      0.000 ...   119.995 secs\n",
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "processed_data_map = {subject: data_processor.process(subject_data_map[subject]) for subject in subject_data_map}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T03:18:34.305036Z",
     "start_time": "2024-03-03T03:18:32.388037Z"
    }
   },
   "id": "45137623ff340cd6",
   "execution_count": 72
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hashing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "256aa1a8ad30843c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Token Normalization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "570588497a87dce4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def normalize_vectors(vectors_to_normalize: typing.List[np.ndarray], token: str) -> typing.List[np.ndarray]:\n",
    "    matrix_generator = auth_biohash.random_token.MatrixGenerator(token)\n",
    "    normalization = auth_biohash.hash.TokenMatrixNormalization(matrix_generator)\n",
    "    return [normalization.normalize(v) for v in vectors_to_normalize]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T03:18:34.320535Z",
     "start_time": "2024-03-03T03:18:34.306047Z"
    }
   },
   "id": "aa0bce2e3282d654",
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "normalized_data_map = {subject: normalize_vectors(processed_data_map[subject], subject_tokens_map[subject]) for subject in processed_data_map}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T03:18:35.731034Z",
     "start_time": "2024-03-03T03:18:34.321535Z"
    }
   },
   "id": "ffdcbb747d2b305e",
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject S01: 20 normalized feature vectors\n",
      "Subject S02: 20 normalized feature vectors\n",
      "Subject S03: 20 normalized feature vectors\n",
      "Subject S04: 20 normalized feature vectors\n",
      "Subject S05: 33 normalized feature vectors\n",
      "Subject S06: 20 normalized feature vectors\n",
      "Subject S07: 20 normalized feature vectors\n",
      "Subject S08: 20 normalized feature vectors\n",
      "Subject S09: 20 normalized feature vectors\n",
      "Subject S10: 20 normalized feature vectors\n",
      "Subject S11: 20 normalized feature vectors\n",
      "Subject S12: 20 normalized feature vectors\n",
      "Subject S13: 20 normalized feature vectors\n",
      "Subject S14: 20 normalized feature vectors\n",
      "Subject S15: 20 normalized feature vectors\n",
      "Subject S16: 20 normalized feature vectors\n",
      "Subject S17: 20 normalized feature vectors\n",
      "Subject S18: 20 normalized feature vectors\n",
      "Subject S19: 20 normalized feature vectors\n",
      "Subject S20: 20 normalized feature vectors\n"
     ]
    }
   ],
   "source": [
    "for subject, vectors in normalized_data_map.items():\n",
    "    print(f'Subject {subject}: {len(vectors)} normalized feature vectors')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T03:18:35.746535Z",
     "start_time": "2024-03-03T03:18:35.732034Z"
    }
   },
   "id": "9ff305324627783c",
   "execution_count": 75
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Encoding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6636cd49c79953a1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def hash_vectors(vectors_to_hash: typing.List[np.ndarray], threshold: float) -> typing.List[auth_biohash.hash.BioHash]:\n",
    "    encoder = feature_encoding.threshold.ThresholdBinaryEncoder(BINARY_THRESHOLD)\n",
    "    return [auth_biohash.hash.BioHash.generate_hash(v, threshold, encoder) for v in vectors_to_hash]\n",
    "\n",
    "\n",
    "def make_map_of_threshold_hashes(vectors_to_hash: typing.List[np.ndarray]) -> typing.Dict[str, typing.List[auth_biohash.hash.BioHash]]:\n",
    "    result = {}\n",
    "    for threshold in AUTHENTICATION_THRESHOLDS:\n",
    "        result[str(threshold)] = hash_vectors(vectors_to_hash, threshold)\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T03:18:35.762035Z",
     "start_time": "2024-03-03T03:18:35.747535Z"
    }
   },
   "id": "a3f0ae55255cbe4b",
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "subject_hashes_map = {subject: make_map_of_threshold_hashes(normalized_data_map[subject]) for subject in normalized_data_map}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T03:18:35.808534Z",
     "start_time": "2024-03-03T03:18:35.763037Z"
    }
   },
   "id": "f752b778aa5d07ef",
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject S01 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S02 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S03 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S04 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S05 BioHash counts (per threshold): 0.1 --> 33, 0.2 --> 33, 0.3 --> 33, 0.4 --> 33, 0.5 --> 33, 0.6 --> 33, 0.7 --> 33, 0.8 --> 33, 0.9 --> 33\n",
      "Subject S06 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S07 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S08 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S09 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S10 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S11 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S12 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S13 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S14 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S15 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S16 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S17 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S18 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S19 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n",
      "Subject S20 BioHash counts (per threshold): 0.1 --> 20, 0.2 --> 20, 0.3 --> 20, 0.4 --> 20, 0.5 --> 20, 0.6 --> 20, 0.7 --> 20, 0.8 --> 20, 0.9 --> 20\n"
     ]
    }
   ],
   "source": [
    "for subject, hashes in subject_hashes_map.items():\n",
    "    hash_counts = ', '.join([f'{hash_threshold} --> {len(hash_instances)}' for hash_threshold, hash_instances in hashes.items()])\n",
    "    print(f'Subject {subject} BioHash counts (per threshold): {hash_counts}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T03:18:35.824034Z",
     "start_time": "2024-03-03T03:18:35.810034Z"
    }
   },
   "id": "1263bc6905481b3e",
   "execution_count": 78
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test Set Assembly"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "532edd13a1d7457e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gathering Test Sets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8d16952f33d8817"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def make_threshold_test_sets(hashes_map: typing.Dict[str, typing.Dict[str, typing.List[auth_biohash.hash.BioHash]]], \n",
    "                             target_subject: str) -> typing.List[ThresholdTestSet]:\n",
    "    threshold_test_sets: typing.Dict[str, ThresholdTestSet] = {\n",
    "        str(threshold): ThresholdTestSet(threshold=threshold, positive_cases=[], negative_cases=[]) \n",
    "        for threshold in AUTHENTICATION_THRESHOLDS\n",
    "    }\n",
    "    for subject in hashes_map:\n",
    "        for threshold in hashes_map[subject]:\n",
    "            test_set = threshold_test_sets[threshold]\n",
    "            if subject == target_subject:\n",
    "                test_set.positive_cases.extend(hashes_map[subject][threshold])\n",
    "            else:\n",
    "                test_set.negative_cases.extend(hashes_map[subject][threshold])\n",
    "    return list(threshold_test_sets.values())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T03:18:35.839538Z",
     "start_time": "2024-03-03T03:18:35.825034Z"
    }
   },
   "id": "247b0c7a4a4fffa8",
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "subject_test_sets = [\n",
    "    SubjectTestSet(subject, make_threshold_test_sets(subject_hashes_map, subject)) \n",
    "    for subject in subject_hashes_map\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T03:18:35.855034Z",
     "start_time": "2024-03-03T03:18:35.841037Z"
    }
   },
   "id": "dfc60522e59349bb",
   "execution_count": 80
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generating Hash Tests"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f30295b6e9df77a1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def make_hash_tests(test_set: ThresholdTestSet) -> typing.List[HashTest]:\n",
    "    tests = []\n",
    "    number_of_positive = len(test_set.positive_cases)\n",
    "    half_point = math.floor(number_of_positive / 2)\n",
    "    initial_cases = test_set.positive_cases[:half_point]\n",
    "    should_match_cases = test_set.positive_cases[half_point:]\n",
    "    should_not_match_cases: typing.List[auth_biohash.hash.BioHash] = random.sample(test_set.negative_cases, half_point)\n",
    "    for sample, comparison in zip(initial_cases, should_match_cases):\n",
    "        tests.append(HashTest(True, (sample, comparison)))\n",
    "    for sample, comparison in zip(initial_cases, should_not_match_cases):\n",
    "        tests.append(HashTest(False, (sample, comparison)))\n",
    "    return tests\n",
    "\n",
    "def make_threshold_tests_map(subject_tests: typing.List[SubjectTestSet]) -> typing.Dict[str, typing.List[HashTest]]:\n",
    "    threshold_tests = {str(threshold): [] for threshold in AUTHENTICATION_THRESHOLDS}\n",
    "    for subject_test in subject_tests:\n",
    "        for threshold_test_data in subject_test.threshold_tests:\n",
    "            threshold_tests[str(threshold_test_data.threshold)].extend(\n",
    "                make_hash_tests(threshold_test_data)\n",
    "            )\n",
    "    return threshold_tests"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T03:18:35.870534Z",
     "start_time": "2024-03-03T03:18:35.856535Z"
    }
   },
   "id": "fc2f37c53e848aa0",
   "execution_count": 81
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "threshold_tests_map = make_threshold_tests_map(subject_test_sets)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T03:18:35.886034Z",
     "start_time": "2024-03-03T03:18:35.871535Z"
    }
   },
   "id": "a3ed7544a11e44ae",
   "execution_count": 82
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Execute Tests"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7640bc382687a81"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def run_threshold_tests(test_data: typing.Dict[str, typing.List[HashTest]]) -> typing.Dict[str, float]:\n",
    "    results = {}\n",
    "    for threshold in test_data:\n",
    "        hits = 0\n",
    "        for test in test_data[threshold]:\n",
    "            is_hit = test.run_test()\n",
    "            if is_hit:\n",
    "                hits += 1\n",
    "        results[threshold] = (hits / len(test_data[threshold])) * 100\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T03:18:35.901526Z",
     "start_time": "2024-03-03T03:18:35.887035Z"
    }
   },
   "id": "1646044c6bc81daa",
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.1, Accuracy: 99.51456310679612%\n",
      "Threshold: 0.2, Accuracy: 99.75728155339806%\n",
      "Threshold: 0.3, Accuracy: 99.75728155339806%\n",
      "Threshold: 0.4, Accuracy: 97.81553398058253%\n",
      "Threshold: 0.5, Accuracy: 70.14563106796116%\n",
      "Threshold: 0.6, Accuracy: 51.45631067961165%\n",
      "Threshold: 0.7, Accuracy: 50.0%\n",
      "Threshold: 0.8, Accuracy: 50.0%\n",
      "Threshold: 0.9, Accuracy: 50.0%\n"
     ]
    }
   ],
   "source": [
    "test_results_map = run_threshold_tests(threshold_tests_map)\n",
    "for threshold_type, accuracy in test_results_map.items():\n",
    "    print(f'Threshold: {threshold_type}, Accuracy: {accuracy}%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T03:18:35.932526Z",
     "start_time": "2024-03-03T03:18:35.902528Z"
    }
   },
   "id": "ae0bcfac89488908",
   "execution_count": 84
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
